---
title: "Porfolio Website v2"
description: "This is the site you are currently looking at. I built it because I felt my old one was kind of boring and lackluster. This one is built with Astro, hosted on GCP and is much easier to add pages to."
pubDate: "Jul 15, 2025"
image: "/images/previews/site.png"
skills: ["Front End Development", "Back-End/Cloud Development", "GCP", "JavaScript", "Astro", "HTML", "Python", "Render", "Supabase", "PostgreSQL"]
---
# Portfolio Website v2
## Overview
<strong>Motivation:</strong> I wanted to build a more impressive portfolio website that had more quality-of-life features, so I did. I also wanted to build this completely from scratch, so I could make everything the way I wanted to.  

<strong>Tech Stack:</strong> GCP (Google Cloud, Google Cloud Scheduler), JavaScript, Astro, HTML, Python, Render, Supabase, PostgreSQL

My old portfolio website got the job done, but wasn't particularly impressive and was tedious to manage. This time around, I made the front-end a lot more modular by storing data in JSON and Markdown files. This makes it MUCH easier to edit and add to the site. I also wanted to give it some more flavor so, I added the terminal, an engagement tracker (click here), wrote an incoming network load simulator, attached a database and hosted it on the cloud. I hope you like it! 

<strong>Source Code: </strong> </strong><a href="https://github.com/Krish-Gandhi/PortfolioWebsite" target="_blank" rel="noopener noreferrer">GitHub Repository</a>

---
## Notes and Design Decisions
### Astro

I chose Astro for this project because I wanted to use something similar to the way GitHub Pages natively uses Jekyll. However, I wanted to host this page on GCP. After doing some research, I found that I could use Astro to type these blog posts as Markdown files and generate them on the site. This makes adding pages MUCH quicker and more convienient. 

### Supabase and PostgreSQL

Admittedly, I did use this project as an excuse to use PostgreSQL because I had never used it before. I chose Supabase here because of its generous free tier for running a PostgreSQL database. I use this database to store information for the engagement tracker on this site. Below is the database schema I am using.

```{sql}
create table public."Engagement" (
  slug character varying not null,
  views bigint null default '0'::bigint,
  constraint Engagement_pkey primary key (slug)
) TABLESPACE pg_default;

create table public."ViewerLogs" (
  id bigint generated by default as identity not null,
  slug character varying not null,
  time timestamp without time zone null default (now() AT TIME ZONE 'utc-6'::text),
  "sentIP" character varying null,
  constraint ViewerLogs_pkey primary key (id),
  constraint PastDayViews_slug_fkey foreign KEY (slug) references "Engagement" (slug) on update CASCADE
) TABLESPACE pg_default;
```

### Render, FastAPI and Uvicorn

The backend for this site is very simple is written in Python using FastAPI and Uvicorn. It is mainly just used for interacting with the database to read and update engagement data. Render has a great free tier for this (750 free instance hours per month). I am also using Render as a way to not eat up my GCP monthly free tier.

### GCP - Hosting and Load Simulation

I choose to host a majority of my personal projects on GCP because I do a majority of my professional work on AWS. So, I like to mix it up and learn different technologies. 

The main problem with Render's free tier is that the instance for my backend server will shutdown if it is left idle for 16 minutes. (Note: Render's documentatation says it is 15 minutes, but checking the instance logs says 16 minutes.) This will lead to cold starts on API calls, which can create 2-3 minutes of latency. This is not awful in principle, but it is annoying. To combat this, I needed to ping the server to keep it awake. At first, I was running a simple cron job on GitHub Actions, but I quickly realized their cron jobs are unreliable in timing. They are extremly inconsistent and do not follow the schedule that is required for this use case. Instead, I am using Google Cloud Scheduler to ping the backend server every 10 minutes. This will simulate network load and keep the instance awake, preventing cold starts.

I also use Google Build Triggers to re-build and deploy this site everytime I push to the GitHub repo. This speeds up the development cycle a ton, and allows me to make quick changes very easily.